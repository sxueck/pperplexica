# Perplexica Configuration File
# This file contains all the configuration options for Perplexica.
# Copy this file to 'config.toml' in your root directory and modify the values as needed.

[GENERAL]
# Similarity measure for document comparison and ranking
# Options: "cosine" (recommended for most cases) or "dot" (dot product)
SIMILARITY_MEASURE = "cosine"

# How long to keep Ollama models loaded into memory
# Format: number + time unit (s, m, h)
# Examples: "5m" (5 minutes), "1h" (1 hour), "30s" (30 seconds)
# Use "-1m" instead of -1 for unlimited (not recommended for production)
KEEP_ALIVE = "5m"

# Hide the settings panel in the UI (useful for public deployments)
# Set to true to hide settings, false to show them
HIDE_SETTINGS = false

# Chat history storage method
# "sqlite": Store chat history on server (persistent across sessions)
# "local": Store chat history in browser only (lost when browser data is cleared)
LIBRARY_STORAGE = "sqlite"

# Search mode configuration - Define which search engines to use for each optimization mode
# This allows you to customize which sources are used based on your needs and available API keys
[SEARCH_MODES]
# Speed mode: Fastest search, uses minimal resources
# Recommended for quick queries where speed is more important than comprehensiveness
SPEED = ["SEARXNG"]

# Balanced mode: Good balance between speed and quality
# Uses multiple sources for better coverage while maintaining reasonable response times
BALANCED = ["SEARXNG", "TAVILY"]

# Quality mode: Most comprehensive search, uses all available sources
# Best for complex queries where thoroughness is more important than speed
QUALITY = ["SEARXNG", "TAVILY", "BOCHAAI"]

# Available search engines:
# - SEARXNG: Self-hosted meta search engine (free, requires setup)
# - TAVILY: AI-powered search API (paid service, good quality)
# - BOCHAAI: BochaAI search service (paid service, real-time results)

# Language Model API Configurations
# Add your API keys here for the models you want to use

[MODELS.OPENAI]
# OpenAI API key for GPT models (gpt-3.5-turbo, gpt-4, etc.)
# Get your key from: https://platform.openai.com/api-keys
API_KEY = ""

[MODELS.GROQ]
# Groq API key for fast inference of open-source models
# Get your key from: https://console.groq.com/keys
# Supports models like Llama, Mixtral, Gemma
API_KEY = ""

[MODELS.ANTHROPIC]
# Anthropic API key for Claude models (claude-3-sonnet, claude-3-haiku, etc.)
# Get your key from: https://console.anthropic.com/
API_KEY = ""

[MODELS.GEMINI]
# Google Gemini API key for Gemini models
# Get your key from: https://aistudio.google.com/app/apikey
API_KEY = ""

[MODELS.CUSTOM_OPENAI]
# Custom OpenAI-compatible API configuration
# Useful for local deployments or alternative providers like Together AI, Fireworks AI
API_KEY = ""           # API key for the custom endpoint
API_URL = ""           # Base URL (e.g., "https://api.together.xyz/v1")
MODEL_NAME = ""        # Model name (e.g., "meta-llama/Llama-2-70b-chat-hf")

[MODELS.OLLAMA]
# Ollama configuration for local model hosting
# Download and run models locally for privacy and cost savings
API_URL = ""           # Ollama API URL (e.g., "http://localhost:11434" or "http://host.docker.internal:11434" for Docker)
RERANK_MODEL = ""      # Reranking model for improving search results (e.g., "bge-reranker-base")

[MODELS.DEEPSEEK]
# DeepSeek API key for DeepSeek models
# Get your key from: https://platform.deepseek.com/api_keys
API_KEY = ""

[MODELS.LM_STUDIO]
# LM Studio configuration for local model serving
# LM Studio provides a local OpenAI-compatible API
API_URL = ""           # LM Studio API URL (e.g., "http://localhost:1234" or "http://host.docker.internal:1234" for Docker)

# Search Engine API Configurations
# Configure the search engines you want to use

[API_ENDPOINTS.SEARXNG]
# SearxNG is a free, open-source metasearch engine
# You need to host your own instance or use a public one
# Setup guide: https://docs.searxng.org/
SEARXNG = ""           # SearxNG API URL (e.g., "http://localhost:8080" or "https://your-searxng-instance.com")

[API_ENDPOINTS.TAVILY]
# Tavily provides AI-optimized search results
# Sign up at: https://tavily.com/
# Offers real-time search with AI-powered result optimization
API_KEY = ""           # Your Tavily API key

[API_ENDPOINTS.BOCHAAI]
# BochaAI provides real-time search capabilities
# Sign up at: https://bocha.ai/
# Offers fast, real-time search results with good coverage
API_KEY = ""           # Your BochaAI API key

# Configuration Examples and Use Cases:

# Example 1: Local-only setup (no external APIs required)
# [SEARCH_MODES]
# SPEED = ["SEARXNG"]
# BALANCED = ["SEARXNG"]
# QUALITY = ["SEARXNG"]
# [MODELS.OLLAMA]
# API_URL = "http://localhost:11434"
# [API_ENDPOINTS.SEARXNG]
# SEARXNG = "http://localhost:8080"

# Example 2: Hybrid setup (local models + external search)
# [MODELS.OLLAMA]
# API_URL = "http://localhost:11434"
# [API_ENDPOINTS.TAVILY]
# API_KEY = "your-tavily-key"
# [SEARCH_MODES]
# SPEED = ["SEARXNG"]
# BALANCED = ["SEARXNG", "TAVILY"]
# QUALITY = ["SEARXNG", "TAVILY"]

# Example 3: Full cloud setup
# [MODELS.OPENAI]
# API_KEY = "your-openai-key"
# [API_ENDPOINTS.TAVILY]
# API_KEY = "your-tavily-key"
# [API_ENDPOINTS.BOCHAAI]
# API_KEY = "your-bochaai-key"
# [SEARCH_MODES]
# SPEED = ["TAVILY"]
# BALANCED = ["TAVILY", "BOCHAAI"]
# QUALITY = ["SEARXNG", "TAVILY", "BOCHAAI"]

# Example 4: Cost-optimized setup (minimize API costs)
# [SEARCH_MODES]
# SPEED = ["SEARXNG"]           # Free search only
# BALANCED = ["SEARXNG"]        # Free search only
# QUALITY = ["SEARXNG", "TAVILY"]  # Add one paid service for quality mode only
# [MODELS.GROQ]                 # Use Groq for faster, cheaper inference
# API_KEY = "your-groq-key"

# Content Guardrail Configuration
# This section configures the content moderation and categorization system
# When enabled, all content submissions to Spaces will be checked for safety and proper categorization
[GUARDRAIL_MODEL]
# Enable or disable the guardrail system
# Set to true to enable content checking before allowing submissions to Spaces
# Set to false to disable guardrail checks (content will be accepted without moderation)
ENABLED = true

# API key for the guardrail model endpoint
# This should be a valid API key for your chosen AI service (OpenAI, Anthropic, etc.)
# Example: "sk-..." for OpenAI, or your custom provider's API key format
API_KEY = ""

# Base URL for the guardrail model API
# This should be an OpenAI-compatible API endpoint
# Examples:
# - "https://api.openai.com/v1" (OpenAI)
# - "https://api.anthropic.com/v1" (Anthropic - if they support OpenAI format)
# - "https://api.together.xyz/v1" (Together AI)
# - "http://localhost:11434/v1" (Local Ollama with OpenAI compatibility)
API_URL = ""

# Model name to use for content moderation
# This should be a model capable of understanding and analyzing content for safety
# Examples:
# - "gpt-3.5-turbo" or "gpt-4" (OpenAI)
# - "meta-llama/Llama-2-70b-chat-hf" (Together AI)
# - "llama2:7b" (Local Ollama)
# Choose a model that balances accuracy with cost/speed for your use case
MODEL_NAME = ""

# How the Guardrail System Works:
# 1. When a user tries to share content to Spaces, the system first sends the content to the guardrail model
# 2. The model analyzes the content for:
#    - Safety: Checks for harmful, inappropriate, or unsafe material
#    - Category accuracy: Verifies if the content matches the specified category
# 3. Based on the analysis, the system either:
#    - Allows the submission (content is safe and properly categorized)
#    - Rejects the submission (content violates safety guidelines)
#    - Adjusts the category (content is safe but miscategorized)
# 4. Users receive feedback about why content was rejected or if categories were adjusted

# Security Considerations:
# - The guardrail model has access to all content being shared
# - Ensure your API keys are kept secure and have appropriate permissions
# - Consider using a dedicated model/API key specifically for content moderation
# - Monitor API usage to prevent unexpected costs from high-volume submissions

# Performance Impact:
# - Each submission requires an additional API call to the guardrail model
# - This adds latency to the sharing process (typically 1-3 seconds)
# - Consider the cost implications of running all submissions through a paid API
# - For high-volume deployments, consider using local models or caching strategies